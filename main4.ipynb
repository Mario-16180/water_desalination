{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from src.environments import water_desalination_environment_v2\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant = water_desalination_environment_v2.env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.Activation = F.leaky_relu\n",
    "        self.affine1 = nn.Linear(6, 16)\n",
    "        self.affine2 = nn.Linear(16, 16)\n",
    "        self.affine3 = nn.Linear(16, 16)\n",
    "        self.affine4 = nn.Linear(16, 16)\n",
    "        self.affine5 = nn.Linear(16, 3)\n",
    "    # modify layers, fully connected layers, activation function, output layer, more neurons\n",
    "    def forward(self, x):\n",
    "        x = self.Activation(self.affine1(x))\n",
    "        x = self.Activation(self.affine2(x))\n",
    "        x = self.Activation(self.affine3(x))\n",
    "        x = self.Activation(self.affine4(x))\n",
    "        action_scores = (self.affine5(x))\n",
    "        prob = F.softmax(action_scores, dim=1)\n",
    "        return prob\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.Activation = F.leaky_relu\n",
    "        self.fc1 = nn.Linear(6, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.Activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, actor, critic, BS = 8):\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    values = []\n",
    "    for _ in range(BS):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        rewards_one_game = []\n",
    "        log_probs_one_game = []\n",
    "        states_one_game = []\n",
    "        values_one_game = []\n",
    "        while not done:\n",
    "            state = torch.from_numpy(state * np.array([4.159547242912022, 4, 100, 2, 1440, 22000]) ** -1).float().unsqueeze(0) # Normalization is included in the casting to tensor\n",
    "            probs = actor(state)\n",
    "            value = critic(state)\n",
    "            m = Categorical(probs)\n",
    "            action = m.sample()\n",
    "            log_prob = m.log_prob(action)\n",
    "            state, reward, done, _ = env.step(action.item())\n",
    "            rewards_one_game.append(reward)\n",
    "            log_probs_one_game.append(log_prob)\n",
    "            states_one_game.append(state)\n",
    "            values_one_game.append(value)\n",
    "        rewards.append(rewards_one_game)\n",
    "        log_probs.append(log_probs_one_game)\n",
    "        values.append(values_one_game)\n",
    "        states.append(states_one_game)\n",
    "    return states, rewards, log_probs, values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic with causality trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "actor_ct = Actor()\n",
    "actor_optim_ct = optim.Adam(actor_ct.parameters()) #, lr=1e-2\n",
    "critic_ct = Critic()\n",
    "critic_optim_ct = optim.Adam(critic_ct.parameters(), lr = 1e-2) #, 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]/var/folders/ts/6y1n3jld49lbmhkghmw7zksr0000gn/T/ipykernel_1542/1426897016.py:14: RuntimeWarning: invalid value encountered in reciprocal\n",
      "  state = torch.from_numpy(state * np.array([4.159547242912022, 4, 100, 2, 1440, 22000]) ** -1).float().unsqueeze(0) # Normalization is included in the casting to tensor\n",
      "100%|██████████| 3000/3000 [6:00:26<00:00,  7.21s/it]  \n"
     ]
    }
   ],
   "source": [
    "valueEstimate_training_AC_ct = []\n",
    "rewardHistory_training_AC_ct = []\n",
    "ANorm2=[]\n",
    "CNorm2=[]\n",
    "for it in tqdm(range(3000)):\n",
    "  states, rewards, log_probs, values = rollout(plant, actor_ct, critic_ct)\n",
    "  R = np.array([np.sum(episode_rewards) for episode_rewards in rewards])\n",
    "  rewardHistory_training_AC_ct.append(np.mean(R))\n",
    "\n",
    "  # R = []\n",
    "  # rewardSum = np.array(rewards)[0]*0\n",
    "  # for r in reversed(rewards):\n",
    "  #   rewardSum += r\n",
    "  #   R.insert(0,rewardSum+0.0)\n",
    "\n",
    "  # actor_loss = -torch.mean(logProbSum*torch.tensor(R))\n",
    "  values_detached = [torch.stack(episode_values).squeeze().detach() for episode_values in values]\n",
    "  actor_loss = -torch.mean(torch.stack([torch.sum(torch.stack(log_probs[i]).squeeze() * (R[i] - values_detached[i])) for i in range(len(log_probs))]))\n",
    "\n",
    "  # actor_loss = -torch.mean(torch.stack([torch.sum(torch.stack(log_probs[i]).squeeze() * (R[i] - torch.stack(values[i]).squeeze().detach())) for i in range(len(log_probs))]))\n",
    "  actor_optim_ct.zero_grad()\n",
    "  actor_loss.backward()\n",
    "  actor_optim_ct.step()\n",
    "  # critic loss for different size of rollout episodes\n",
    "\n",
    "  # critic_loss = torch.mean((torch.stack([torch.stack(episode_values).squeeze() for episode_values in values]).squeeze() - torch.tensor(R))**2)\n",
    "  # Make list of tensors a tensor from values\n",
    "  values = [torch.stack(episode_values).squeeze() for episode_values in values]\n",
    "\n",
    "  critic_loss = torch.sum(torch.stack([torch.sum((R[i] - values[i]) ** 2) for i in range(len(log_probs))])) / sum([len(episode) for episode in rewards])\n",
    "  critic_optim_ct.zero_grad()\n",
    "  critic_loss.backward()\n",
    "  critic_optim_ct.step()\n",
    "\n",
    "  nor = 0.0\n",
    "  for w in actor_ct.parameters():\n",
    "    nor+= torch.norm(w.grad)**2\n",
    "  ANorm2.append(nor)\n",
    "\n",
    "  nor = 0.0\n",
    "  for w in critic_ct.parameters():\n",
    "    nor+= torch.norm(w.grad)**2\n",
    "  CNorm2.append(nor)\n",
    "  \n",
    "  V = torch.tensor([vals[0] for vals in values])\n",
    "  valueEstimate_training_AC_ct.append(torch.mean(V).item())\n",
    "  torch.save(actor_ct.state_dict(), \"model_weights.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot moving average of reward history with window size 10\n",
    "def plot_moving_average(values, window=10):\n",
    "    weights = np.repeat(1.0, window)/window\n",
    "    sma = np.convolve(values, weights, 'valid')\n",
    "    plt.plot(sma)\n",
    "# plot_moving_average(rewardHistory_training_AC)\n",
    "#plt.plot(rewardHistory_training_AC)\n",
    "#plt.plot(valueEstimate_training_AC,'r--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = plant.reset()\n",
    "done = False\n",
    "rewardHistory = []\n",
    "actionHistory = []\n",
    "while not done:\n",
    "    state = torch.from_numpy(state * np.array([4.159547242912022, 4, 100, 2, 1440, 22000]) ** -1).float().unsqueeze(0)\n",
    "    probs = actor_ct(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    state, reward, done, _ = plant.step(action.item())\n",
    "    rewardHistory.append(reward)\n",
    "    actionHistory.append(action.item())\n",
    "# Cumulative reward\n",
    "# plt.plot(np.cumsum(rewardHistory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved in results/exp6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def save_results(experiment_number, reward_history_testing):\n",
    "    # Create directory for experiment if it does not exist\n",
    "    results_dir = 'results'\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.mkdir(results_dir)\n",
    "    experiment_dir = f'{results_dir}/exp{experiment_number}'\n",
    "    if not os.path.exists(experiment_dir):\n",
    "        os.mkdir(experiment_dir)\n",
    "\n",
    "    plt.plot(np.cumsum(rewardHistory))\n",
    "    plt.title('action')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Reward')\n",
    "    plot_filename = f'{experiment_dir}/action_history.png'\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    \n",
    "    # Save plot as image\n",
    "    plot_moving_average(rewardHistory_training_AC_ct)\n",
    "    plt.title('Moving Average of Reward History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Reward')\n",
    "    plot_filename = f'{experiment_dir}/reward_history.png'\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    \n",
    "    # Save text results\n",
    "    data = np.column_stack((actionHistory.count(0), actionHistory.count(2.0), actionHistory.count(1.0), len(rewardHistory)))\n",
    "    np.savetxt(os.path.join(experiment_dir, \"results.txt\"), data, fmt='%-10.5f', delimiter='\\t', header='Fossil Fuel\\tBattery\\tSolar Panel\\tReward History')\n",
    "\n",
    "    plt.semilogy(ANorm2)\n",
    "    plt.semilogy(CNorm2)\n",
    "    plt.title('The Euclidean Norm of Gradients')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Norm')\n",
    "    plt.legend()\n",
    "    plot_filename = f'{experiment_dir}/Norm_history.png'\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f'Results saved in {experiment_dir}')\n",
    "\n",
    "\n",
    "# Get next experiment number\n",
    "experiment_number = 1\n",
    "results_dir = 'results'\n",
    "if os.path.exists(results_dir):\n",
    "    experiment_number = max([int(name.split('exp')[1]) for name in os.listdir(results_dir) if name.startswith('exp')]) + 1\n",
    "\n",
    "# Save results\n",
    "save_results(experiment_number, rewardHistory_training_AC_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ts/6y1n3jld49lbmhkghmw7zksr0000gn/T/ipykernel_1326/2143771249.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrewardHistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mactionHistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plant' is not defined"
     ]
    }
   ],
   "source": [
    "state = plant.reset()\n",
    "done = False\n",
    "rewardHistory = []\n",
    "actionHistory = []\n",
    "while not done:\n",
    "    state = torch.from_numpy(state * np.array([4.159547242912022, 4, 100, 2, 1440, 22000]) ** -1).float().unsqueeze(0)\n",
    "    probs = actor_ct(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    state, reward, done, _ = plant.step(action.item())\n",
    "    rewardHistory.append(reward)\n",
    "    actionHistory.append(action.item())\n",
    "# Cumulative reward\n",
    "plt.plot(np.cumsum(rewardHistory))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionHistory.count(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actionHistory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ts/6y1n3jld49lbmhkghmw7zksr0000gn/T/ipykernel_1542/798899149.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactionHistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'actionHistory' is not defined"
     ]
    }
   ],
   "source": [
    "actionHistory.count(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionHistory.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1157"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rewardHistory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "water_desalination",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
